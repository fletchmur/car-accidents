{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef4ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a76fd",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the population data for percapita calculations as well as representative sampling\n",
    "population = pd.read_excel(\"ignorefolder/population.xlsx\")\n",
    "\n",
    "population = (population\n",
    "    .drop(columns=\"Unnamed: 1\")\n",
    "    .iloc[:, [0, 5]]\n",
    "    .rename(columns= {population.columns[0]: \"Geographic Area\", population.columns[6]: \"Population\"})\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "population[\"Geographic Area\"] = population[\"Geographic Area\"].str.lstrip(\".\").str.lower()\n",
    "#dropping puerto rico\n",
    "population = population.drop(index=[56, 1, 2, 3, 4]).reset_index(drop=True)\n",
    "\n",
    "#calculating proportion of population\n",
    "population['Proportion'] = population[\"Population\"] / population[\"Population\"].iloc[0]\n",
    "total_data = 1000000\n",
    "population[\"Sample Counts\"] = np.floor(population[\"Proportion\"] * total_data)\n",
    "population = population.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "state_to_abbrev = {\n",
    "    \"alabama\": \"AL\",\n",
    "    \"alaska\": \"AK\",\n",
    "    \"arizona\": \"AZ\",\n",
    "    \"arkansas\": \"AR\",\n",
    "    \"american samoa\": \"AS\",\n",
    "    \"california\": \"CA\",\n",
    "    \"colorado\": \"CO\",\n",
    "    \"connecticut\": \"CT\",\n",
    "    \"delaware\": \"DE\",\n",
    "    \"district of columbia\": \"DC\",\n",
    "    \"florida\": \"FL\",\n",
    "    \"georgia\": \"GA\",\n",
    "    \"guam\": \"GU\",\n",
    "    \"hawaii\": \"HI\",\n",
    "    \"idaho\": \"ID\",\n",
    "    \"illinois\": \"IL\",\n",
    "    \"indiana\": \"IN\",\n",
    "    \"iowa\": \"IA\",\n",
    "    \"kansas\": \"KS\",\n",
    "    \"kentucky\": \"KY\",\n",
    "    \"louisiana\": \"LA\",\n",
    "    \"maine\": \"ME\",\n",
    "    \"maryland\": \"MD\",\n",
    "    \"massachusetts\": \"MA\",\n",
    "    \"michigan\": \"MI\",\n",
    "    \"minnesota\": \"MN\",\n",
    "    \"mississippi\": \"MS\",\n",
    "    \"missouri\": \"MO\",\n",
    "    \"montana\": \"MT\",\n",
    "    \"nebraska\": \"NE\",\n",
    "    \"nevada\": \"NV\",\n",
    "    \"new hampshire\": \"NH\",\n",
    "    \"new jersey\": \"NJ\",\n",
    "    \"new mexico\": \"NM\",\n",
    "    \"new york\": \"NY\",\n",
    "    \"north carolina\": \"NC\",\n",
    "    \"north dakota\": \"ND\",\n",
    "    \"northern mariana islands\": \"MP\",\n",
    "    \"ohio\": \"OH\",\n",
    "    \"oklahoma\": \"OK\",\n",
    "    \"oregon\": \"OR\",\n",
    "    \"pennsylvania\": \"PA\",\n",
    "    \"puerto rico\": \"PR\",\n",
    "    \"rhode island\": \"RI\",\n",
    "    \"south carolina\": \"SC\",\n",
    "    \"south dakota\": \"SD\",\n",
    "    \"tennessee\": \"TN\",\n",
    "    \"texas\": \"TX\",\n",
    "    \"trust territories\": \"TT\",\n",
    "    \"utah\": \"UT\",\n",
    "    \"vermont\": \"VT\",\n",
    "    \"virginia\": \"VA\",\n",
    "    \"virgin islands\": \"VI\",\n",
    "    \"washington\": \"WA\",\n",
    "    \"west virginia\": \"WV\",\n",
    "    \"wisconsin\": \"WI\",\n",
    "    \"wyoming\": \"WY\"\n",
    "}\n",
    "population[\"State\"] = population[\"Geographic Area\"].map(state_to_abbrev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['Severity', 'State', 'Temperature(F)', 'Wind_Chill(F)', \n",
    "                'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', \n",
    "                'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity', \n",
    "                'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
    "                'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming',\n",
    "                'Traffic_Signal', 'Turning_Loop']\n",
    "\n",
    "\n",
    "df = pd.read_csv('ignorefolder/US_Accidents_March23.csv')\n",
    "df = df[cols_to_keep]\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116a1da",
   "metadata": {},
   "source": [
    "### Representative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(population[[\"State\", \"Sample Counts\"]], on=\"State\", how='left')\n",
    "grouped_by_state = df.groupby(\"State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b43a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(988706, 25)\n"
     ]
    }
   ],
   "source": [
    "def sample_group(g):\n",
    "    n = int(g[\"Sample Counts\"].iloc[0])\n",
    "    n = min(g.shape[0], n)\n",
    "    return g.sample(n, replace=False)\n",
    "\n",
    "sampled_dataset = grouped_by_state.apply(sample_group)\n",
    "print(sampled_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa1a5a",
   "metadata": {},
   "source": [
    "### Removing the group by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = sampled_dataset.drop(columns='Sample Counts')\n",
    "sampled_dataset = sampled_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_one_vals = ['rain', 'wind', 'drizzle', 'shower', 'sand', 'blowing', 'thunder', 't-storm', 'heavy']\n",
    "plus_two_vals = ['heavy', 'freezing', 'ice', 'hail', 'sleet']\n",
    "plus_three_vals = ['tornado']\n",
    "\n",
    "score_arrays = [(1, plus_one_vals), (2, plus_two_vals), (3, plus_three_vals)]\n",
    "vals_to_add = []\n",
    "\n",
    "pairs = []\n",
    "scores = []\n",
    "\n",
    "for val in sampled_dataset['Weather_Condition'].unique():\n",
    "    severity_score = 0\n",
    "    if type(val) != str:\n",
    "        pairs.append((val, 1))\n",
    "        continue\n",
    "\n",
    "    for score, array in score_arrays:\n",
    "        pattern = r'(' + '|'.join(map(re.escape, array)) + r')'\n",
    "        matches = re.findall(pattern, val, re.IGNORECASE)\n",
    "        severity_score += score * len(matches)\n",
    "    \n",
    "    severity_score = 2 if severity_score > 2 else severity_score\n",
    "    pairs.append((val, severity_score))\n",
    "\n",
    "weather_condition_map = dict(pairs)\n",
    "sampled_dataset['Weather_Condition_Score'] = sampled_dataset[\"Weather_Condition\"].map(weather_condition_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f640b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Weather_Condition_Score\n",
       "0    869094\n",
       "1    106237\n",
       "2     13375\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset[\"Weather_Condition_Score\"].unique()\n",
    "sampled_dataset[\"Weather_Condition_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ad588",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset[\"Weather_Condition\"] = sampled_dataset[\"Weather_Condition_Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1e036",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a788c",
   "metadata": {},
   "source": [
    "### Saving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset.to_csv(\"cleaned_US_Accidents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2205dd1",
   "metadata": {},
   "source": [
    "sampled_dataset = pd.get_dummies(sampled_dataset, columns=[\"Wind_Direction\", \"Weather_Condition\"])\n",
    "sampled_dataset.drop(columns=[\"Sample Counts\"])\n",
    "print(sampled_dataset.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
